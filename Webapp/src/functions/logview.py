import pandas as pd
import os
import glob
import numpy as np
from pathlib import Path
import time
from datetime import datetime  
import tempfile
import shutil

BASE_DIR = os.path.dirname(os.path.abspath(__file__))



# ---------- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏ß‡πá‡∏ö ----------
import pandas as pd
import os
from typing import Union, List

def find_input_files(input_pattern: str):
    if os.path.isdir(input_pattern):
        files = glob.glob(os.path.join(input_pattern, "*.txt")) + \
        glob.glob(os.path.join(input_pattern, "*.TXT"))
        files = list(set(os.path.abspath(f) for f in files))
    else:
        files = glob.glob(input_pattern)
    return files

def load_and_parse_file(input_file: str) -> pd.DataFrame:
    try:
        with open(input_file, 'r', encoding='latin-1') as file:
            lines = file.readlines()
    except Exception as e:
        print(f"Error reading {input_file}: {e}")
        return pd.DataFrame()
    rows = []
    max_values_len = 0
    for line in lines:
        parts = line.strip().split('\t')
        if len(parts) < 3:
            continue
        timestamp, data_type, data_values = parts[0], parts[1], parts[2]
        try:
            date_part, time_part = timestamp.split(' ')
            time_part = time_part.replace('AM', '').replace('PM', '').strip()
        except ValueError:
            continue
        values = data_values.split(',')
        if len(values) > max_values_len:
            max_values_len = len(values)
        row = [date_part, time_part, data_type] + values
        rows.append(row)
    if not rows:
        return pd.DataFrame()
    max_len = 6 + max_values_len
    for row in rows:
        row += [''] * (max_len - len(row))
    columns = ['date', 'time', 'step', 'frame', 'G', 'No_strip'] + [f'value_{i}' for i in range(1, max_values_len + 1)]
    df = pd.DataFrame(rows, columns=columns)
    pattern = r'(FU|FR|FA|FW|FN|FJ|F1|F2|F3|F4|F5|F6|F7|F8|F9|F0)(\w{4})'
    df['frame'] = df['frame'].astype(str)
    df['frame'] = df['frame'].str.extract(pattern).fillna('').agg(''.join, axis=1)
    return df

def extract_pro_and_speed(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame()
    df_pro = df[df['step'] == 'PRO'].copy()
    if df_pro.empty:
        return pd.DataFrame()
    df_pro['speed'] = None
    for idx in df_pro.index:
        speed_value = None
        pos = df.index.get_loc(idx)
        for j in range(pos + 1, len(df)):
            if df.loc[df.index[j], 'step'] == 'CUC':
                if 'value_5' in df.columns and len(df.columns) > df.columns.get_loc('value_5'):
                    speed_value = df.loc[df.index[j], 'value_5']
                break
        df_pro.at[idx, 'speed'] = speed_value
    df_pro['speed'] = pd.to_numeric(df_pro['speed'], errors='coerce')
    df_pro['speed'] = df_pro['speed'] / 10 / 25.4
    df_pro['speed'] = df_pro['speed'].apply(lambda x: int(x) if x % 1 == 0 else round(x, 2))
    return df_pro

def mark_errors(df: pd.DataFrame, df_pro: pd.DataFrame) -> pd.DataFrame:
    if df.empty or df_pro.empty:
        return df_pro
    df_pro['MC'] = None
    pro_indices = df.index[df['step'] == 'PRO'].tolist()
    for i in range(1, len(pro_indices)):
        current_idx = pro_indices[i]
        prev_idx = pro_indices[i - 1]
        start_idx = min(prev_idx, current_idx)
        end_idx = max(prev_idx, current_idx)
        df_slice = df.loc[start_idx:end_idx]
        error_steps = ['ERRSET', 'ERRRCV', 'ERRCLR', 'DMC', 'DMW']
        has_error = df_slice['step'].isin(error_steps).any()
        if has_error and prev_idx in df_pro.index:
            df_pro.at[prev_idx, 'MC'] = 'MC error'
    return df_pro

def insert_blank_rows(df_pro: pd.DataFrame) -> pd.DataFrame:
    if df_pro.empty:
        return df_pro
    new_rows = []
    for i in range(len(df_pro)):
        row = df_pro.iloc[i]
        new_rows.append(row)
        try:
            first_strip = float(row['No_strip']) == 1
        except (ValueError, TypeError):
            first_strip = False
        if first_strip:
            empty_row = pd.Series([None] * len(df_pro.columns), index=df_pro.columns)
            new_rows.append(empty_row)
    df_with_blank = pd.DataFrame(new_rows).reset_index(drop=True)
    return df_with_blank

def calculate_time_diff(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], errors='coerce')
    df['minute'] = df['datetime'] - df['datetime'].shift(-1)
    df['seconds'] = df['minute'].dt.total_seconds()
    df = df[df['seconds'].isna() | ((df['seconds'] >= 0) & (df['seconds'] <= 86400))]
    df['minute'] = df['minute'].astype(str)
    df.loc[df['minute'] == 'NaT', 'minute'] = ''
    df['minute'] = df['minute'].str.replace('0 days ', '')
    df = df.drop(columns=['datetime'])
    return df

def assign_subgroups_and_insert_empty_rows(df, column_strip='No_strip', frame_group='frame'):
    subgroup_id = 0
    subgroups = []
    prev_val = None
    for val in df[column_strip]:
        if pd.isna(val):
            subgroups.append(np.nan)
            prev_val = None
            continue
        if prev_val is None:
            subgroup_id += 1
        elif val > prev_val:
            subgroup_id += 1
        subgroups.append(subgroup_id)
        prev_val = val
    df['subgroup_id'] = subgroups
    result_rows = []
    subgroup_keys = df['subgroup_id'].dropna().unique()
    for group in subgroup_keys:
        group_df = df[df['subgroup_id'] == group].reset_index(drop=True)
        result_rows.append(group_df.iloc[[0]])
        for i in range(1, len(group_df)):
            prev_frame = group_df.loc[i - 1, frame_group]
            curr_frame = group_df.loc[i, frame_group]
            if prev_frame != curr_frame:
                empty_row = pd.DataFrame({col: [np.nan] for col in df.columns})
                result_rows.append(empty_row)
            result_rows.append(group_df.iloc[[i]])
        empty_row = pd.DataFrame({col: [np.nan] for col in df.columns})
        result_rows.append(empty_row)
    result_df = pd.concat(result_rows, ignore_index=True).reset_index(drop=True)
    return result_df

def mark_outlier_subgroups(df, subgroup_col='subgroup_id', no_strip_col='No_strip'):
    outlier_groups = []
    for subgroup, group_df in df.groupby(subgroup_col):
        if 1 not in group_df[no_strip_col].values:
            outlier_groups.append(subgroup)
    df['outlier_subgroup'] = df[subgroup_col].isin(outlier_groups)
    return df

def detect_outliers_combined(df, group_col='frame', value_col='seconds', no_strip_col='No_strip',
                             iqr_factor=1, zscore_threshold=2, min_diff_seconds=90):
    df['is_outlier'] = False
    df_filtered = df[~((df[no_strip_col] == 2) & (df[no_strip_col].shift(-1) == 1))]
    for group_name, group in df_filtered.groupby(group_col):
        values = group[value_col].dropna().values
        if len(values) == 0:
            continue
        median = np.median(values)
        q1 = np.percentile(values, 25)
        q3 = np.percentile(values, 75)
        iqr = q3 - q1
        upper_bound = q3 + iqr_factor * iqr
        mean = np.mean(values)
        std = np.std(values)
        for idx in group.index:
            val = df.loc[idx, value_col]
            if pd.isna(val):
                continue
            if val > upper_bound and abs(val - median) > min_diff_seconds:
                df.at[idx, 'is_outlier'] = True
            elif std > 0:
                z_score = (val - mean) / std
                if z_score > zscore_threshold and abs(val - mean) > min_diff_seconds:
                    df.at[idx, 'is_outlier'] = True
    return df

def add_avg_exclude_outliers_by_frame(
    df, 
    value_col='seconds', 
    group_col='frame', 
    outlier_col='is_outlier',
    outlier_subgroup_col='outlier_subgroup',
    outlier_mc='MC'
):
    df['avg_ex_outliers'] = pd.NA
    df['count_avg'] = pd.NA
    df['count_outliers'] = pd.NA
    for frame_val in df[group_col].dropna().unique():
        group_df = df[df[group_col] == frame_val]
        good_values = group_df[(group_df[outlier_col] != True) & (group_df[outlier_subgroup_col] != True) & (group_df[outlier_mc] != 'MC error')][value_col].dropna()
        if len(good_values) < 5:
            continue
        avg_val = good_values.mean()
        count_avg_val = len(good_values)
        count_all = group_df[value_col].count()
        count_outliers = count_all - count_avg_val
        idx = df[df[group_col] == frame_val].index
        if len(idx) > 0:
            first_idx = idx[0]
            df.at[first_idx, 'avg_ex_outliers'] = round(avg_val, 2)
            df.at[first_idx, 'count_avg'] = count_avg_val
            df.at[first_idx, 'count_outliers'] = count_outliers
    return df

def summarize_by_frame(df):
    summary = df.groupby(['frame', 'speed']).agg({
        'sec/strip': 'first',
    }).reset_index()
    return summary

def process_single_file_complete(input_file: str, output_dir: str):
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {input_file}")
    input_path = Path(input_file)
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_path / f"{input_path.stem}_{timestamp}.xlsx"
    
    try:
        df = load_and_parse_file(input_file)
        if df.empty:
            return False, f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {input_file}"
        
        df_pro = extract_pro_and_speed(df)
        if df_pro.empty:
            return False, f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• PRO ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {input_file}"
        
        df_pro = mark_errors(df, df_pro)
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        available_value_cols = [col for col in df_pro.columns if col.startswith('value_')]
        value_cols = available_value_cols[:1] if available_value_cols else []
        selected_cols = ['date', 'time', 'step', 'package', 'frame', 'No_strip'] + value_cols + ['speed','MC']
        existing_cols = [col for col in selected_cols if col in df_pro.columns]
        df_pro = df_pro[existing_cols]
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df_with_blank = insert_blank_rows(df_pro)
        df_time = calculate_time_diff(df_with_blank)
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        for col in ['frame', 'speed','value_1']:
            if col in df_time.columns:
                if col == 'frame':
                    df_time[col] = df_time[col].astype(str).str.strip()
                else:
                    df_time[col] = pd.to_numeric(df_time[col], errors='coerce')
        
        if 'No_strip' in df_time.columns:
            df_time['No_strip'] = pd.to_numeric(df_time['No_strip'], errors='coerce')
        
        df_filtered = df_time[df_time['frame'].notna()]
        if df_filtered.empty:
            return False, f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• frame ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {input_file}"
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df_analyzed = assign_subgroups_and_insert_empty_rows(df_filtered, 'No_strip', 'frame')
        df_analyzed = mark_outlier_subgroups(df_analyzed, 'subgroup_id', 'No_strip')
        df_analyzed = detect_outliers_combined(df_analyzed, 'frame', 'seconds', 'No_strip')
        df_analyzed = add_avg_exclude_outliers_by_frame(df_analyzed, value_col='seconds', group_col='frame')
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Error columns
        if 'outlier_subgroup' in df_analyzed.columns and 'is_outlier' in df_analyzed.columns and 'MC' in df_analyzed.columns:
            df_analyzed['Error'] = (df_analyzed['outlier_subgroup'] | df_analyzed['is_outlier'] | (df_analyzed['MC'] == 'MC error'))
        elif 'outlier_subgroup' in df_analyzed.columns and 'is_outlier' in df_analyzed.columns:
            df_analyzed['Error'] = df_analyzed['outlier_subgroup'] | df_analyzed['is_outlier']
        else:
            df_analyzed['Error'] = False
        
        # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        df_analyzed.drop(columns=['outlier_subgroup', 'is_outlier','MC'], inplace=True, errors='ignore')
        df_analyzed['Error'] = df_analyzed['Error'].apply(lambda x: "MC ERROR" if x else "")
        df_analyzed.drop(columns=['subgroup_id'], inplace=True, errors='ignore')
        df_analyzed['sec/strip'] = df_analyzed['avg_ex_outliers']
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Summary
        summary = summarize_by_frame(df_analyzed)
        df_final = df_analyzed.drop(columns=['avg_ex_outliers'])
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel
        with pd.ExcelWriter(output_file) as writer:
            df_final.to_excel(writer, index=False, sheet_name='Processed_Data')
            summary.to_excel(writer, index=False, sheet_name='Summary')
        
        return True, str(output_file)
        
    except Exception as e:
        return False, f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {input_file}: {str(e)}"

def process_multiple_files_complete(input_pattern: str, output_dir: str):
    files = find_input_files(input_pattern)
    if not files:
        print(f" ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö pattern: {input_pattern}")
        return
    print(f" ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(files)} ‡πÑ‡∏ü‡∏•‡πå")
    print("=" * 60)
    successful = 0
    failed = 0
    start_time = time.time()
    for i, file_path in enumerate(files, 1):
        print(f"[{i}/{len(files)}] ", end="")
        success, message = process_single_file_complete(file_path, output_dir)
        if success:
            print(f" ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {message}")
            successful += 1
        else:
            print(f" ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {message}")
            failed += 1
    end_time = time.time()
    print("\n" + "=" * 60)
    print(f" ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤: {end_time - start_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
    print(f" ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {successful} ‡πÑ‡∏ü‡∏•‡πå, ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß {failed} ‡πÑ‡∏ü‡∏•‡πå")

# ---------- 2. ‡∏£‡∏ß‡∏° Summary ----------

def load_sec_strip_by_frame(filepath, sheet_name='Processed_Data'):
    print(f"         üìÑ ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {os.path.basename(filepath)}")
    
    try:
        df = pd.read_excel(filepath, sheet_name=sheet_name)
        print(f"         ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô sheet '{sheet_name}' ‡πÑ‡∏î‡πâ: {df.shape}")
    except ValueError as e:
        print(f"         ‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö sheet '{sheet_name}', ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô Sheet1")
        try:
            df = pd.read_excel(filepath, sheet_name='Sheet1')
            print(f"         ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô Sheet1 ‡πÑ‡∏î‡πâ: {df.shape}")
        except Exception as e2:
            print(f"         ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô Sheet1: {str(e2)}")
            raise ValueError(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ: {str(e2)}")
    except Exception as e:
        print(f"         ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {str(e)}")
        raise
    
    print(f"         üìã ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ: {list(df.columns)}")
    
    required_cols = ['frame', 'speed', 'sec/strip']
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        print(f"         ‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£: {missing_cols}")
        raise ValueError(f"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£: {missing_cols}")
    
    print(f"         ‚úÖ ‡∏°‡∏µ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£")
    
    df['frame'] = df['frame'].astype(str)
    df['speed'] = pd.to_numeric(df['speed'], errors='coerce')
    df['sec/strip'] = pd.to_numeric(df['sec/strip'], errors='coerce')
    
    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏≠‡∏á
    before_filter = len(df)
    df = df[df['sec/strip'].notna() & df['speed'].notna()]
    after_filter = len(df)
    
    print(f"         üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏≠‡∏á: {before_filter} ‡πÅ‡∏ñ‡∏ß")
    print(f"         üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á: {after_filter} ‡πÅ‡∏ñ‡∏ß")
    
    if df.empty:
        print(f"         ‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á")
    
    return df

def summarize_sec_strip(files_folder, file_list):
    print(f"   üìÅ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å folder: {files_folder}")
    print(f"   üìã ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå: {file_list}")
    
    data = {}
    successful_files = 0
    failed_files = 0
    
    for filename in file_list:
        filepath = os.path.join(files_folder, filename)
        print(f"   üîç ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {filename}")
        
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
            if not os.path.exists(filepath):
                print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {filepath}")
                failed_files += 1
                continue
                
            df = load_sec_strip_by_frame(filepath)
            print(f"      üìä ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ: {df.shape[0]} ‡πÅ‡∏ñ‡∏ß")
            
            if df.empty:
                print(f"      ‚ö†Ô∏è  ‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤: {filename}")
                failed_files += 1
                continue
                
            summary = df.groupby(['frame', 'speed'])['sec/strip'].mean()
            summary.index = summary.index.map(lambda x: f"{x[0]}_speed{x[1]}")
            file_key = os.path.splitext(filename)[0]
            data[file_key] = summary
            
            print(f"      ‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(summary)} ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
            successful_files += 1
            
        except Exception as e:
            print(f"      ‚ùå ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå {filename} : {str(e)}")
            failed_files += 1
            continue
    
    print(f"   üìä ‡∏™‡∏£‡∏∏‡∏õ: ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {successful_files} ‡πÑ‡∏ü‡∏•‡πå, ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß {failed_files} ‡πÑ‡∏ü‡∏•‡πå")
    
    if not data:
        print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏î‡πÜ ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
        return pd.DataFrame()
        
    result_df = pd.DataFrame(data)
    result_df = result_df.sort_index()
    print(f"   ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á result DataFrame: {result_df.shape}")
    
    return result_df

def save_summary(df, output_path):
    df.index.name = "FRAME_STOCK"
    df.to_excel(output_path, index=True)
    print(f"‚úÖ Saved comparison summary to: {output_path}")

# ---------- 3. Export CSV ----------

def remove_outliers(data):
    if not data:
        return []
    arr = np.array(data)
    q1 = np.percentile(arr, 25)
    q3 = np.percentile(arr, 75)
    iqr = q3 - q1
    upper_bound = q3 + 1.5 * iqr
    filtered = arr[(arr <= upper_bound)]
    return filtered.tolist()

def filtered_mean(lst):
    filtered = remove_outliers(lst)
    if len(filtered) == 0:
        return float('nan')
    return sum(filtered) / len(filtered)

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î mapping ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Package group ‚Üí Lead frame
MAPPING = {
    ('QFN', '5.0'): 'Copper ',
    ('QFN', '4.0'): 'Selective PPF',
    ('QFN', '3.0'): 'Full PPF',
}

def analyze_and_export_csv(summary_path, package_path, output_csv):
    df = pd.read_excel(summary_path)
    df2 = pd.read_excel(package_path)
    df['non_null_values'] = df.loc[:, df.columns != 'FRAME_STOCK'].apply(
        lambda row: row.dropna().tolist(), axis=1)
    df = df[['FRAME_STOCK', 'non_null_values']]
    df['TIME/STRIP'] = df['non_null_values'].apply(filtered_mean)
    df = df[['FRAME_STOCK', 'TIME/STRIP']]
    df['SPEED'] = df['FRAME_STOCK'].astype(str).str[-3:]
    df['X'] = df['FRAME_STOCK'].astype(str).str[0:6]
    df = df[['X', 'SPEED', 'TIME/STRIP', 'FRAME_STOCK']]
    df = df.drop(columns='FRAME_STOCK')
    df['TIME/STRIP'] = df['TIME/STRIP'].round(2)
    df.rename(columns={'X': 'FRAME_STOCK'}, inplace=True)
    df.rename(columns={'SPEED': 'SPEED (IPS)'},inplace=True)
    df_merged = pd.merge(df, df2[['FRAME_STOCK', 'PACKAGE_CODE']], on='FRAME_STOCK', how='left')
    df_merged.to_csv(output_csv, index=False)
    print(f"‚úÖ Exported summary CSV: {output_csv}")

def analyze_and_export_csv_from_df(summary_df, package_path, output_csv):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV
    """
    print("üìä ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å CSV...")
    
    df = summary_df.reset_index()
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå index ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 'FRAME_STOCK'
    if df.columns[0] != 'FRAME_STOCK':
        df = df.rename(columns={df.columns[0]: 'FRAME_STOCK'})
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• package
    print(f"üìÅ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• package ‡∏à‡∏≤‡∏Å: {package_path}")
    df2 = pd.read_excel(package_path, sheet_name="Export Worksheet")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå package
    print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå package...")
    required_cols = ['FRAME_STOCK', 'PACKAGE_CODE', 'Package size ', 'Package group', 'Frame type ', 'Unit/strip']
    available_cols = ['FRAME_STOCK']  # FRAME_STOCK ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ
    
    for col in required_cols[1:]:  # ‡∏Ç‡πâ‡∏≤‡∏° FRAME_STOCK
        if col in df2.columns:
            available_cols.append(col)
            print(f"   ‚úÖ ‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {col}")
        else:
            print(f"   ‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {col}")
    
    print(f"üìä ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ merge: {available_cols}")
    
    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
    df['non_null_values'] = df.loc[:, df.columns != 'FRAME_STOCK'].apply(
        lambda row: row.dropna().tolist(), axis=1)
    df = df[['FRAME_STOCK', 'non_null_values']]
    df['TIME/STRIP'] = df['non_null_values'].apply(filtered_mean)
    df = df[['FRAME_STOCK', 'TIME/STRIP']]
    df['SPEED (IPS)'] = df['FRAME_STOCK'].astype(str).str[-3:]
    df['X'] = df['FRAME_STOCK'].astype(str).str[0:6]
    df = df[['X', 'SPEED (IPS)', 'TIME/STRIP', 'FRAME_STOCK']]
    df = df.drop(columns='FRAME_STOCK')
    df['TIME/STRIP'] = df['TIME/STRIP'].round(2)
    df.rename(columns={'X': 'FRAME_STOCK'}, inplace=True)
    
    # Merge ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• package
    print("üîó ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ö package data...")
    df_merged = pd.merge(
        df,
        df2[available_cols],
        on='FRAME_STOCK',
        how='left'
    )
    
    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ MAPPING (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á)
    if 'Frame type ' in df_merged.columns and 'Package group' in df_merged.columns:
        print("üîß ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Frame type ‡∏ï‡∏≤‡∏° mapping...")
        df_merged['Frame type '] = df_merged.apply(
            lambda row: MAPPING.get((str(row['Package group']), str(row['SPEED (IPS)'])), row['Frame type ']),
            axis=1
        )
    else:
        print("‚ö†Ô∏è  ‡∏Ç‡πâ‡∏≤‡∏° MAPPING ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Process (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Package group)
    if 'Package group' in df_merged.columns:
        print("‚öôÔ∏è ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Process...")
        df_merged['Process'] = None
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Package group
        df_merged['Package group'] = df_merged['Package group'].astype(str).str.strip().str.upper()
        # ‡πÅ‡∏õ‡∏•‡∏á SPEED ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        df_merged['SPEED (IPS)'] = pd.to_numeric(df_merged['SPEED (IPS)'], errors='coerce')

        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≥‡∏´‡∏ô‡∏î Process ‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug
        print("   üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≥‡∏´‡∏ô‡∏î Process:")
        debug_summary = df_merged.groupby(['Package group', 'SPEED (IPS)']).size().reset_index(name='count')
        for _, row in debug_summary.iterrows():
            print(f"      - Package group: '{row['Package group']}' | Speed: {row['SPEED (IPS)']} | ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {row['count']}")

        # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Process - ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö SLP ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        print("   üîç ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Process ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SLP ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô...")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÅ‡∏¢‡∏Å‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        condition_slp_speed5 = (df_merged['SPEED (IPS)'] == 5) & (df_merged['Package group'] == 'SLP')
        condition_slp_speed3 = (df_merged['SPEED (IPS)'] == 3) & (df_merged['Package group'] == 'SLP')
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç
        print(f"      - SLP Speed 5: {condition_slp_speed5.sum()} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        print(f"      - SLP Speed 3: {condition_slp_speed3.sum()} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Process ‡∏î‡πâ‡∏ß‡∏¢ .loc[] ‡πÅ‡∏ó‡∏ô np.select()
        df_merged.loc[condition_slp_speed5, 'Process'] = 'Full Cut'
        df_merged.loc[condition_slp_speed3, 'Process'] = 'Step Cut'
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î Process
        process_summary = df_merged.groupby(['Package group', 'SPEED (IPS)', 'Process']).size().reset_index(name='count')
        print("   üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î Process:")
        for _, row in process_summary.iterrows():
            process_value = row['Process'] if pd.notna(row['Process']) else 'None'
            print(f"      - {row['Package group']} | Speed {row['SPEED (IPS)']} ‚Üí {process_value} ({row['count']} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")
            
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö QFN ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÑ‡∏î‡πâ Process ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
        qfn_with_process = df_merged[(df_merged['Package group'] == 'QFN') & (df_merged['Process'].notna())]
        if not qfn_with_process.empty:
            print(f"   ‚ö†Ô∏è  ‡∏û‡∏ö QFN ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ Process ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {len(qfn_with_process)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            for _, row in qfn_with_process.iterrows():
                print(f"      - {row['FRAME_STOCK']}: Package group = '{row['Package group']}', Speed = {row['SPEED (IPS)']}, Process = '{row['Process']}'")
        else:
            print("   ‚úÖ QFN ‡πÑ‡∏°‡πà‡∏°‡∏µ Process (‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)")
            
    else:
        print("‚ö†Ô∏è  ‡∏Ç‡πâ‡∏≤‡∏° Process ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Package group")
        # ‡πÅ‡∏õ‡∏•‡∏á SPEED ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢
        df_merged['SPEED (IPS)'] = pd.to_numeric(df_merged['SPEED (IPS)'], errors='coerce')
    
    # ‚úÖ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ group_and_average_across_frames_unique_frame
    print("üéØ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡πâ‡∏≤‡∏° frame...")
    df_final = group_and_average_across_frames_unique_frame(df_merged)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV
    print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV: {output_csv}")
    df_final.to_csv(output_csv, index=False)
    print(f"‚úÖ Exported summary CSV: {output_csv}")
    
    return df_final

def group_and_average_across_frames_unique_frame(df_merged):
    print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢...")
    
    # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°
    potential_grouping_cols = [
        'Package size ',
        'Package group',
        'Frame type ',
        'Unit/strip',
        'SPEED (IPS)'
    ]
    
    grouping_cols = [col for col in potential_grouping_cols if col in df_merged.columns]
    
    if 'SPEED (IPS)' not in grouping_cols:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå SPEED (IPS) ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°")
        return df_merged
    
    print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {df_merged.shape[0]} ‡πÅ‡∏ñ‡∏ß")
    
    # ‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö duplicate
    print("üîÑ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• FRAME_STOCK ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô...")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö duplicate ‡∏Å‡πà‡∏≠‡∏ô
    duplicated_frames = df_merged[df_merged.duplicated(subset=['FRAME_STOCK'], keep=False)]
    if not duplicated_frames.empty:
        print(f"‚ö†Ô∏è  ‡∏û‡∏ö FRAME_STOCK ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô: {duplicated_frames['FRAME_STOCK'].nunique()} ‡∏ï‡∏±‡∏ß")
        duplicate_analysis = {}
        for frame in duplicated_frames['FRAME_STOCK'].unique():
            frame_data = duplicated_frames[duplicated_frames['FRAME_STOCK'] == frame]
            time_values = frame_data['TIME/STRIP'].dropna().tolist()
            duplicate_analysis[frame] = {
                'count': len(frame_data),
                'time_values': time_values,
                'non_null_count': len(time_values)
            }
            print(f"   - {frame}: {len(frame_data)} ‡πÅ‡∏ñ‡∏ß ‚Üí TIME/STRIP: {time_values}")
    
    # ‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    def smart_aggregation(group):
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö TIME/STRIP: ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô NaN
        time_values = group['TIME/STRIP'].dropna()
        if len(time_values) > 0:
            time_result = time_values.mean()
        else:
            time_result = np.nan
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏∑‡πà‡∏ô‡πÜ: ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô NaN
        result = {}
        for col in df_merged.columns:
            if col == 'TIME/STRIP':
                result[col] = time_result
            elif col == 'FRAME_STOCK':
                result[col] = group[col].iloc[0]  # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
            else:
                # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô NaN
                non_null_values = group[col].dropna()
                if len(non_null_values) > 0:
                    result[col] = non_null_values.iloc[0]
                else:
                    result[col] = group[col].iloc[0]
        
        return pd.Series(result)
    
    # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° FRAME_STOCK
    df_unique = df_merged.groupby('FRAME_STOCK').apply(smart_aggregation).reset_index(drop=True)
    
    print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°: {df_unique.shape[0]} ‡πÅ‡∏ñ‡∏ß")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á TIME/STRIP
    if not duplicated_frames.empty:
        print("üìä ‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏° TIME/STRIP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö FRAME_STOCK ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥:")
        for frame, info in duplicate_analysis.items():
            if info['non_null_count'] > 1:
                new_value = df_unique[df_unique['FRAME_STOCK'] == frame]['TIME/STRIP'].iloc[0]
                if not pd.isna(new_value):
                    print(f"   - {frame}: {info['time_values']} ‚Üí {round(new_value, 2)} (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏à‡∏≤‡∏Å {info['non_null_count']} ‡∏Ñ‡πà‡∏≤)")
    
    # ‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° FRAME_STOCK ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TIME/STRIP
    frames_without_time = df_unique[df_unique['TIME/STRIP'].isna()]['FRAME_STOCK'].tolist()
    if frames_without_time:
        print(f"‚ö†Ô∏è  FRAME_STOCK ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TIME/STRIP: {len(frames_without_time)} ‡∏ï‡∏±‡∏ß")
        for frame in frames_without_time[:5]:  # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà 5 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
            print(f"   - {frame}")
        if len(frames_without_time) > 5:
            print(f"   ... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(frames_without_time) - 5} ‡∏ï‡∏±‡∏ß")
    
    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡πà‡∏≠‡πÑ‡∏õ...
    group_avg_map = {}
    group_before_map = {}
    group_after_map = {}
    total_groups = 0
    processed_groups = 0
    total_outliers_removed = 0
    excluded_frames = []
    excluded_reasons = []
    
    print("üîç ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
    print("=" * 80)
    
    for group_key, group_df in df_unique.groupby(grouping_cols):
        total_groups += 1
        group_name = " | ".join([f"{col}={val}" for col, val in zip(grouping_cols, group_key)])
        total_frames_in_group = len(group_df)
        
        # ‡∏ô‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ TIME/STRIP
        values = group_df['TIME/STRIP'].dropna().tolist()
        frames_with_data = group_df[group_df['TIME/STRIP'].notna()]['FRAME_STOCK'].tolist()
        frames_without_data = group_df[group_df['TIME/STRIP'].isna()]['FRAME_STOCK'].tolist()

        # ‚úÖ ‡πÉ‡∏ä‡πâ len(values) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        group_before_map[group_key] = len(values)
    
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å FRAME_STOCK ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TIME/STRIP
        for frame in frames_without_data:
            excluded_frames.append(frame)
            excluded_reasons.append(f"‡∏Å‡∏•‡∏∏‡πà‡∏°: {group_name} | ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TIME/STRIP")

        # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Before_Outlier ‡πÅ‡∏•‡∏∞ After_Outlier ‡πÄ‡∏õ‡πá‡∏ô 0
        if len(values) < 2:
            print(f"‚ùå ‡∏Å‡∏•‡∏∏‡πà‡∏°: {group_name}")
            print(f"   üìä Frame Stock ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_frames_in_group} ‡∏ï‡∏±‡∏ß")
            print(f"   üìä ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TIME/STRIP: {len(values)} ‡∏Ñ‡πà‡∏≤")
            print(f"   üìã FRAME_STOCK ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {frames_with_data}")
            if frames_without_data:
                print(f"   ‚ùå FRAME_STOCK ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {frames_without_data}")
            
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÅ‡∏°‡πâ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢
            if len(values) == 1:
                group_avg_map[group_key] = values[0]
                group_after_map[group_key] = 1  # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ
                print(f"   ‚úÖ ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß: {values[0]}")
            elif len(values) == 0:
                group_avg_map[group_key] = np.nan
                group_after_map[group_key] = np.nan  # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 0 ‡πÄ‡∏õ‡πá‡∏ô NaN
                print(f"   ‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡πÉ‡∏ä‡πâ NaN")
            else:
                group_avg_map[group_key] = np.mean(values)
                group_after_map[group_key] = len(values)  # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á
                print(f"   ‚úÖ ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏≠‡∏á: {np.mean(values)}")
            
            print(f"   üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {len(values)}/{group_after_map[group_key]} (‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•/‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î)")
            print()
            
        else:
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• outliers
            q1 = np.percentile(values, 25)
            q3 = np.percentile(values, 75)
            iqr = q3 - q1
            lower = q1 - 1.5 * iqr
            upper = q3 + 1.5 * iqr
            
            filtered = [v for v in values if lower <= v <= upper]
            outliers = [v for v in values if v < lower or v > upper]
            
            if filtered:
                avg_val = round(np.mean(filtered), 2)
                group_avg_map[group_key] = avg_val
                group_after_map[group_key] = len(filtered)  # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á
                processed_groups += 1
                total_outliers_removed += len(outliers)
                
                print(f"‚úÖ ‡∏Å‡∏•‡∏∏‡πà‡∏°: {group_name}")
                print(f"   üìä Frame Stock ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_frames_in_group} ‡∏ï‡∏±‡∏ß")
                print(f"   üìä ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TIME/STRIP: {len(values)} ‡∏Ñ‡πà‡∏≤")
                print(f"   üìà ‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏Å‡∏ï‡∏¥: {round(lower, 2)} - {round(upper, 2)}")
                print(f"   ‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {len(filtered)} ‡∏Ñ‡πà‡∏≤ ‚Üí {filtered}")
                
                if outliers:
                    print(f"   ‚ùå Outliers ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å: {len(outliers)} ‡∏Ñ‡πà‡∏≤ ‚Üí {outliers}")
                    print(f"   üìä ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î: {round(len(outliers)/len(values)*100, 1)}%")
                else:
                    print(f"   ‚ú® ‡πÑ‡∏°‡πà‡∏°‡∏µ outliers")
                
                print(f"   üéØ ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: {avg_val}")
                print(f"   üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {len(values)}/{len(filtered)} (‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•/‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î)")
                print()
            else:
                print(f"‚ö†Ô∏è  ‡∏Å‡∏•‡∏∏‡πà‡∏°: {group_name}")
                print(f"   üìä Frame Stock ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_frames_in_group} ‡∏ï‡∏±‡∏ß")
                print(f"   üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(values)} ‡∏Ñ‡πà‡∏≤ ‚Üí {values}")
                print(f"   ‚ùå ‡∏ó‡∏∏‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô outliers")
                print(f"   üìà ‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏Å‡∏ï‡∏¥: {round(lower, 2)} - {round(upper, 2)}")
                
                group_avg_map[group_key] = round(np.mean(values), 2)
                group_after_map[group_key] = len(values)  # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 0 ‡πÄ‡∏õ‡πá‡∏ô len(values) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏î‡∏¥‡∏ö
                print(f"   ‚ö†Ô∏è  ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏î‡∏¥‡∏ö: {round(np.mean(values), 2)}")
                print(f"   üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {len(values)}/{len(values)} (‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•/‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î)")
                print()

    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ
    print("=" * 80)
    print(f"üìà ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•:")
    print(f"   üî¢ ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_groups} ‡∏Å‡∏•‡∏∏‡πà‡∏°")
    print(f"   ‚úÖ ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ: {processed_groups} ‡∏Å‡∏•‡∏∏‡πà‡∏°")
    print(f"   ‚ùå ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ: {total_groups - processed_groups} ‡∏Å‡∏•‡∏∏‡πà‡∏°")
    print(f"   üóëÔ∏è  Outliers ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_outliers_removed} ‡∏Ñ‡πà‡∏≤")
    print(f"   üìä ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {round(processed_groups/total_groups*100, 1)}%")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô FRAME_STOCK ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡∏°‡∏≤‡∏Ñ‡∏¥‡∏î
    if excluded_frames:
        print(f"\n‚ùå FRAME_STOCK ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡∏°‡∏≤‡∏Ñ‡∏¥‡∏î: {len(excluded_frames)} ‡∏ï‡∏±‡∏ß")
        print("=" * 80)
        for i, (frame, reason) in enumerate(zip(excluded_frames, excluded_reasons), 1):
            print(f"   {i:2d}. {frame} ‚Üí {reason}")
        print("=" * 80)
    else:
        print(f"\n‚úÖ FRAME_STOCK ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡∏°‡∏≤‡∏Ñ‡∏¥‡∏î‡πÅ‡∏•‡πâ‡∏ß")
    
    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö apply
    def assign_before_count(row):
        key = tuple(row[col] for col in grouping_cols)
        count = group_before_map.get(key, np.nan)
        # ‚úÖ ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 0 - ‡πÉ‡∏ä‡πâ NaN ‡πÅ‡∏ó‡∏ô
        return count if pd.notna(count) and count > 0 else np.nan

    def assign_after_count(row):
        key = tuple(row[col] for col in grouping_cols)
        count = group_after_map.get(key, np.nan)
        # ‚úÖ ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 0 - ‡πÉ‡∏ä‡πâ NaN ‡πÅ‡∏ó‡∏ô
        return count if pd.notna(count) and count > 0 else np.nan

    def assign_avg(row):
        key = tuple(row[col] for col in grouping_cols)
        original_value = row['TIME/STRIP']
        new_value = group_avg_map.get(key, original_value)
        
        if pd.notna(original_value) and pd.notna(new_value) and abs(original_value - new_value) > 0.01:
            change_type = "üìà" if new_value > original_value else "üìâ"
            diff = abs(new_value - original_value)
            before_count = group_before_map.get(key, np.nan)
            after_count = group_after_map.get(key, np.nan)
            print(f"   {change_type} {row['FRAME_STOCK']}: {original_value} ‚Üí {new_value} (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô {round(diff, 2)}) [{before_count}/{after_count}]")
            
        return new_value

    print("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡πà‡∏≤ TIME/STRIP...")
    df_unique['TIME/STRIP'] = df_unique.apply(assign_avg, axis=1)

    print("üìä ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î...")
    df_unique['Before_Outlier'] = df_unique.apply(assign_before_count, axis=1)
    df_unique['After_Outlier'] = df_unique.apply(assign_after_count, axis=1)

    print("‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢")
    return df_unique

def LOGVIEW(input_path, output_dir):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå LOGVIEW
    """
    print(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• LOGVIEW")
    print(f"üìÅ Input: {input_path}")
    print(f"üìÅ Output: {output_dir}")
    
    # 1. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå input ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå .xlsx ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
    print("üìä ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå input...")
    before_files = set(f for f in os.listdir(output_dir) if f.lower().endswith('.xlsx'))
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
        process_multiple_files_complete(input_path, temp_dir)
        # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
        temp_files = set(f for f in os.listdir(temp_dir) if f.lower().endswith('.xlsx'))
        
        if not temp_files:
            print(" ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå .xlsx ‡πÉ‡∏´‡∏°‡πà")
            return

        print(f" ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà {len(temp_files)} ‡πÑ‡∏ü‡∏•‡πå")
        
        # ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å temp_dir ‡πÑ‡∏õ‡∏¢‡∏±‡∏á output_dir
        import shutil
        new_files = []
        for filename in temp_files:
            src = os.path.join(temp_dir, filename)
            dst = os.path.join(output_dir, filename)
            shutil.copy2(src, dst)
            new_files.append(filename)
            print(f"   ‚úÖ ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å: {filename}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß
    if not new_files:
        print(" ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ")
        return

    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á summary DataFrame
    print("üìä ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á summary...")
    try:
        summary_df = summarize_sec_strip(output_dir, new_files)
        print(f"   ‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• summary: {summary_df.shape}")
        
        if summary_df.empty:
            print("   ‚ùå Summary DataFrame ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤")
            return
            
    except Exception as e:
        print(f"   ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á summary: {str(e)}")
        return
    
    # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå package
    print("üìä ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå package...")
    package_path = os.path.join(BASE_DIR, "..", "data_MAP", "export package and frame stock Rev.06.xlsx")
    package_path = os.path.abspath(package_path)
    
    print(f"   üìÅ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö package path: {package_path}")
    
    if not os.path.exists(package_path):
        print("   ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå export package and frame stock Rev.04.xlsx ‡πÉ‡∏ô data_MAP")
        print(f"   üìÅ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö directory: {os.path.dirname(package_path)}")
        if os.path.exists(os.path.dirname(package_path)):
            upload_files = os.listdir(os.path.dirname(package_path))
            print(f"   üìã ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô Upload folder: {upload_files}")
        return
    else:
        print(f"   ‚úÖ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå package: {package_path}")
    
    # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Summary.csv ‡∏î‡πâ‡∏ß‡∏¢ timestamp
    print("üìä ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_csv = os.path.join(output_dir, f"Summary_{timestamp}.csv")
    
    try:
        # ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ group_and_average_across_frames_unique_frame ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        final_df = analyze_and_export_csv_from_df(summary_df, package_path, output_csv)
        print(f"üéâ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        print(f"   üìÑ ‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {output_csv}")
        print(f"   üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: {final_df.shape[0]} ‡πÅ‡∏ñ‡∏ß")
        if os.path.exists(output_csv):
            return output_csv
        else:
            print(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {output_csv}")
            return None
    except Exception as e:
        print(f"   ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á CSV: {str(e)}")
        return None





