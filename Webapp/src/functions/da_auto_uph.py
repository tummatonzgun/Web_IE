import pandas as pd
import numpy as np
import os
import json
from datetime import datetime

def apply_zscore(df, uph_col):
    """‡∏ï‡∏±‡∏î outliers ‡∏î‡πâ‡∏ß‡∏¢ Z-Score (¬±3 std)"""
    mean = df[uph_col].mean()
    std = df[uph_col].std()
    if std == 0:
        return df
    z_scores = (df[uph_col] - mean) / std
    filtered = df[(z_scores >= -3) & (z_scores <= 3)].copy()
    filtered['Outlier_Method'] = 'Z-Score ¬±3'
    return filtered

def apply_iqr(df, uph_col):
    """‡∏ï‡∏±‡∏î outliers ‡∏î‡πâ‡∏ß‡∏¢ IQR"""
    Q1 = df[uph_col].quantile(0.25)
    Q3 = df[uph_col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    filtered = df[(df[uph_col] >= lower) & (df[uph_col] <= upper)].copy()
    filtered['Outlier_Method'] = 'IQR'
    return filtered

def has_outlier(df, uph_col):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö outliers"""
    Q1 = df[uph_col].quantile(0.25)
    Q3 = df[uph_col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return ((df[uph_col] < lower) | (df[uph_col] > upper)).sum() > 0

def remove_outliers_auto(df_model, uph_col, max_iter=20):
    """‡∏ï‡∏±‡∏î outliers ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
    df_model[uph_col] = pd.to_numeric(df_model[uph_col], errors='coerce')
    df_model = df_model.dropna(subset=[uph_col])

    if len(df_model) < 15:
        df_model['Outlier_Method'] = '‡πÑ‡∏°‡πà‡∏ï‡∏±‡∏î (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢)'
        return df_model

    current_df = df_model.copy()
    for i in range(max_iter):
        z_df = apply_zscore(current_df, uph_col)
        if not has_outlier(z_df, uph_col):
            z_df['Outlier_Method'] = f'Z-Score Loop √ó{i+1}'
            return z_df

        iqr_df = apply_iqr(z_df, uph_col)
        if not has_outlier(iqr_df, uph_col):
            iqr_df['Outlier_Method'] = f'IQR Loop √ó{i+1}'
            return iqr_df
        current_df = iqr_df

    current_df['Outlier_Method'] = f'IQR-Z-Score Loop √ó{max_iter}+'
    return current_df

def get_column_names(df):
    """‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"""
    col_map = {col.lower(): col for col in df.columns}
    
    uph_col = col_map.get('uph')
    if not uph_col:
        raise KeyError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå UPH")
    
    model_col = col_map.get('machine model') or col_map.get('machine_model')
    if not model_col:
        raise KeyError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Machine Model")
    
    bom_col = col_map.get('bom_no') or col_map.get('bom no')
    if not bom_col:
        raise KeyError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå bom_no")
    
    date_col = None
    for col_name in df.columns:
        if any(keyword in col_name.lower() for keyword in ['date', 'time', '‡∏ß‡∏±‡∏ô', '‡πÄ‡∏ß‡∏•‡∏≤']):
            date_col = col_name
            break
    
    return uph_col, model_col, bom_col, date_col

def load_file(file_path):
    """‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó"""
    if file_path.endswith('.xlsx'):
        return pd.read_excel(file_path, engine='openpyxl')
    elif file_path.endswith('.xls'):
        return pd.read_excel(file_path, engine='xlrd')
    elif file_path.endswith('.csv'):
        return pd.read_csv(file_path)
    elif file_path.endswith('.json'):
        with open(file_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        if isinstance(json_data, list):
            return pd.DataFrame(json_data)
        elif isinstance(json_data, dict):
            for key in ['data', 'results', 'items', 'records']:
                if key in json_data and isinstance(json_data[key], list):
                    return pd.DataFrame(json_data[key])
            return pd.DataFrame([json_data])
    else:
        return pd.read_excel(file_path, engine='openpyxl')

def remove_outliers(df):
    """‡∏ï‡∏±‡∏î outliers ‡∏ï‡∏≤‡∏°‡∏Å‡∏•‡∏∏‡πà‡∏°"""
    uph_col, model_col, bom_col, _ = get_column_names(df)
    result_dfs = []
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° optn_code ‡πÉ‡∏ô groupby
    for (bom_no, machine_model, optn_code), group_df in df.groupby([bom_col, model_col, 'optn_code']):
        before_count = len(group_df)
        cleaned_group = remove_outliers_auto(group_df, uph_col)
        after_count = len(cleaned_group)
        cleaned_group['DataPoints_Before'] = before_count
        cleaned_group['DataPoints_After'] = after_count
        result_dfs.append(cleaned_group)
    
    return pd.concat(result_dfs, ignore_index=True)

def process_date_column(df):
    """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"""
    _, _, _, date_col = get_column_names(df)
    
    if not date_col:
        print("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà")
        return df
    
    print(f"‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {date_col}")
    df['date_time_start'] = pd.to_datetime(df[date_col], errors='coerce')
    df['date_time_start'] = df['date_time_start'].dt.strftime('%Y/%m/%d')
    
    invalid_dates = df['date_time_start'].isna().sum()
    if invalid_dates > 0:
        print(f"‡∏û‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {invalid_dates} ‡πÅ‡∏ñ‡∏ß")
        df = df.dropna(subset=['date_time_start'])
    
    return df

def get_date_range(df, start_date=None, end_date=None):
    """‡πÑ‡∏î‡πâ‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"""
    if start_date and end_date:
        return start_date, end_date
    
    max_date = df['date_time_start'].max()
    min_date = df['date_time_start'].min()
    print(f"‡πÉ‡∏ä‡πâ‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {min_date} ‡∏ñ‡∏∂‡∏á {max_date}")
    return min_date, max_date

def filter_by_date_range(df, start_date, end_date):
    """‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"""
    filtered_df = df[df['date_time_start'].between(start_date, end_date)].copy()
    
    if len(filtered_df) == 0:
        raise Exception("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")
    
    print(f"‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(filtered_df)}/{len(df)} ‡πÅ‡∏ñ‡∏ß")
    return filtered_df

def calculate_group_average(df, start_date, end_date):
    uph_col, model_col, bom_col, _ = get_column_names(df)
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° optn_code ‡πÉ‡∏ô groupby
    grouped = df.groupby([bom_col, model_col, 'optn_code'], as_index=False).agg({uph_col: 'mean'})
    grouped[uph_col] = grouped[uph_col].round(3)
    other_cols = ['operation', 'optn_code'] + (['DataPoints_Before', 'DataPoints_After'] if 'DataPoints_Before' in df.columns else [])
    if other_cols:
        firsts = df.groupby([bom_col, model_col, 'optn_code'], as_index=False)[other_cols].first()
        grouped = pd.merge(grouped, firsts, on=[bom_col, model_col, 'optn_code'], how='left')
    print(f"=== ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ UPH ({start_date} ‡∏ñ‡∏∂‡∏á {end_date}) ===")
    print(grouped)
    return grouped

def save_results(df_cleaned, grouped_average, start_date, end_date, output_dir):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"""
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    date_range = f"{start_date.replace('/', '')}_to_{end_date.replace('/', '')}"
    
    cleaned_file = os.path.join(output_dir, f"cleaned_data_{date_range}_{timestamp}.xlsx")
    average_file = os.path.join(output_dir, f"group_average_{date_range}_{timestamp}.xlsx")
    
    df_cleaned.to_excel(cleaned_file, index=False, engine='openpyxl')
    grouped_average.to_excel(average_file, index=False, engine='openpyxl')
    
    print(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå: {cleaned_file}")
    print(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå: {average_file}")
    
    return cleaned_file, average_file

def process_die_attack_data(file_path, start_date=None, end_date=None):
    """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Die Attack"""
    print("=== ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Die Attack ===")
    
    df = load_file(file_path)
    print(f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {len(df)} ‡πÅ‡∏ñ‡∏ß")
    
    df = process_date_column(df)
    
    if start_date and end_date:
        start_date = start_date.replace("-", "/")
        end_date = end_date.replace("-", "/")
    else:
        start_date, end_date = get_date_range(df)
    
    df_filtered = filter_by_date_range(df, start_date, end_date)
    
    print("‡∏ï‡∏±‡∏î outliers...")
    df_cleaned = remove_outliers(df_filtered)
    print(f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î outliers: {len(df_cleaned)} ‡πÅ‡∏ñ‡∏ß")
    
    grouped_average = calculate_group_average(df_cleaned, start_date, end_date)
    
    return df_cleaned, grouped_average, start_date, end_date

def preview_date_range(file_path):
    """‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå"""
    try:
        df = load_file(file_path)
        print(f"‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(df):,} ‡πÅ‡∏ñ‡∏ß")
        
        date_cols = [col for col in df.columns 
                    if any(keyword in col.lower() for keyword in ['date', 'time', '‡∏ß‡∏±‡∏ô', '‡πÄ‡∏ß‡∏•‡∏≤'])]
        
        if not date_cols:
            print("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà")
            return None
        
        date_col = date_cols[0]
        df['temp_date'] = pd.to_datetime(df[date_col], errors='coerce')
        valid_dates = df.dropna(subset=['temp_date'])
        
        if len(valid_dates) == 0:
            print("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
            return None
        
        min_date = valid_dates['temp_date'].min()
        max_date = valid_dates['temp_date'].max()
        
        print(f"‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {min_date.strftime('%Y-%m-%d')} ‡∏ñ‡∏∂‡∏á {max_date.strftime('%Y-%m-%d')}")
        print(f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {len(valid_dates):,} ‡πÅ‡∏ñ‡∏ß")
        
        return {
            'min_date': min_date.strftime('%Y-%m-%d'),
            'max_date': max_date.strftime('%Y-%m-%d'),
            'valid_records': len(valid_dates),
            'total_records': len(df)
        }
        
    except Exception as e:
        print(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
        return None

def check_bom_differences(df_map):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÉ‡∏ô BOM ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô dict ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á"""
    
    # ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
    columns_to_check = [
        'Die Size1', 'Wafer Size1', 'Die Size2', 'Wafer Size2', 
        'Die Size3', 'Wafer Size3', 'Die Size4', 'Wafer Size4',
        'Die Size5', 'Wafer Size5', 'Epoxy 1', 'Epoxy 2', 'Epoxy 3', 
        'Epoxy 4', 'Epoxy 5', 'Wire1', '#of Wire1', '#of Bump1',
        'Wire2', '#of Wire2', '#of Bump2', 'Compound 1', 'Compound 2', 'Solder 1'
    ]
    
    # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
    existing_columns = [col for col in columns_to_check if col in df_map.columns]
    
    print(f"\nüîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÉ‡∏ô BOM")
    print(f"‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: {len(existing_columns)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
    
    if not existing_columns:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ")
        return {}
    
    # ‡∏´‡∏≤ BOM column
    bom_col = None
    for col in df_map.columns:
        if 'bom' in col.lower():
            bom_col = col
            break
    
    if not bom_col:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå BOM")
        return {}
    
    bom_differences = {}
    
    # ‡πÄ‡∏ä‡πá‡∏Ñ‡πÅ‡∏ï‡πà‡∏•‡∏∞ BOM
    for bom_no in df_map[bom_col].unique():
        if pd.isna(bom_no):
            continue
            
        bom_data = df_map[df_map[bom_col] == bom_no]
        
        if len(bom_data) <= 1:
            bom_differences[bom_no] = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏ñ‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"
            continue
        
        differences = []
        
        for col in existing_columns:
            unique_values = bom_data[col].dropna().unique()
            
            if len(unique_values) > 1:
                values_str = ', '.join([str(v) for v in unique_values[:3]])
                if len(unique_values) > 3:
                    values_str += f'... (+{len(unique_values)-3})'
                differences.append(f"{col}({len(unique_values)}‡∏Ñ‡πà‡∏≤: {values_str})")
        
        if differences:
            bom_differences[bom_no] = " | ".join(differences)
            print(f"üìã BOM: {bom_no} - ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á: {len(differences)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
        else:
            bom_differences[bom_no] = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏´‡∏°‡∏î"
            print(f"‚úÖ BOM: {bom_no} - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏´‡∏°‡∏î")
    
    return bom_differences

def map_data(average_file):
    """Map ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Part bom pkg ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå data_MAP"""
    print("=== Map ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ===")
    
    try:
        # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå average
        df_average = pd.read_excel(average_file, engine='openpyxl')
        print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• average: {len(df_average)} ‡πÅ‡∏ñ‡∏ß")

        # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå mapping
        current_dir = os.path.dirname(os.path.abspath(__file__))
        map_folder = os.path.join(current_dir, "..", "data_MAP")

        mapping_file = os.path.join(map_folder, "Part bom pkg.xlsx")

        if not os.path.exists(mapping_file):
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {mapping_file}")
            return average_file

        # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå mapping ‡πÅ‡∏£‡∏Å
        df_map = pd.read_excel(mapping_file, engine='openpyxl')
        print(f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• mapping: {len(df_map)} ‡πÅ‡∏ñ‡∏ß")

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÉ‡∏ô BOM
        bom_differences = check_bom_differences(df_map)

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        required_cols = ["Package Code", "Cust Code", "Product Number", "bom_no"]
        missing_cols = [col for col in required_cols if col not in df_map.columns]
        
        if missing_cols:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {missing_cols}")
            return average_file

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Device ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å
        df_map["Device"] = df_map[["Package Code", "Cust Code", "Product Number"]].astype(str).agg('_'.join, axis=1)
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å
        map_cols = ["bom_no", "Device"]
        if "#of Die" in df_map.columns:
            map_cols.append("#of Die")
        elif "of Die" in df_map.columns:
            map_cols.append("of Die")
        
        df_map_selected = df_map[map_cols]

        # Merge ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å
        df_merged = df_average.merge(df_map_selected, on="bom_no", how="left")
        print(f"‚úÖ Map ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(df_merged)} ‡πÅ‡∏ñ‡∏ß")

        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå BOM_Differences
        df_merged['BOM_Differences'] = df_merged['bom_no'].map(bom_differences).fillna('‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• BOM')
        
        print(f"‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå BOM_Differences ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà map ‡πÅ‡∏•‡πâ‡∏ß
        output_dir = os.path.dirname(average_file)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        mapped_file = os.path.join(output_dir, f"mapped_data_{timestamp}.xlsx")
        
        df_merged.to_excel(mapped_file, index=False, engine='openpyxl')
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà map ‡πÅ‡∏•‡πâ‡∏ß: {mapped_file}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• BOM_Differences
        print(f"\nüìã ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á BOM_Differences:")
        sample_data = df_merged[['bom_no', 'BOM_Differences']].drop_duplicates().head(3)
        for _, row in sample_data.iterrows():
            print(f"   BOM: {row['bom_no']} -> {row['BOM_Differences'][:100]}{'...' if len(str(row['BOM_Differences'])) > 100 else ''}")
        
        return mapped_file

    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ map ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
        return average_file

def DA_AUTO_UPH(file_path, temp_root, start_date=None, end_date=None):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Die Attack"""
    try:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö input type
        if isinstance(file_path, list):
            if len(file_path) == 0:
                print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                return None
            actual_file_path = file_path[0]  # ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å
            print(f"‚ö†Ô∏è ‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå ({len(file_path)} ‡πÑ‡∏ü‡∏•‡πå) ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å: {actual_file_path}")
        else:
            actual_file_path = file_path

        df_cleaned, grouped_average, used_start_date, used_end_date = process_die_attack_data(
            actual_file_path, start_date, end_date)

        cleaned_file, average_file = save_results(
            df_cleaned, grouped_average, used_start_date, used_end_date, temp_root)

        print(f"‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {used_start_date} ‡∏ñ‡∏∂‡∏á {used_end_date}")
        
        if not os.path.exists(average_file):
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå average_file")
            return None

        # Map ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        #mapped_file = map_data(average_file)      
        print(f"üìÅ ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÑ‡∏ü‡∏•‡πå: {average_file}")
        return average_file

    except Exception as e:
        print(f"‚ùå DA_AUTO_UPH error: {e}")
        return None

